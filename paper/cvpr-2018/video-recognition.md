# Video Recognition

## 1. Low-Latency Video Semantic Segmentation

极低延迟性的视频语义分割，商汤科技，Spotlight

### 简介

如何高效的实现视频语义分割是一个极富挑战性的问题。其困难在于：

* 与图像分割相比，视频分割通常涉及更多的数据。比如，视频每秒通常包含15～30帧，分析视频因而需要更多的计算资源；
* 许多实际应用（如自动驾驶）中的视频分割模块需要实现视频分割的低延迟性。

对于视频语义分割任务，大部分现有工作关注如何在每帧计算量和分割精度之间的达到一个平衡点，却并没有深入的思考和探讨算法延迟性这个因素。现有工作可以被大致分为两类：

* 高层特征的时序建模方法
* 中间层特征传播的方法

前者主要在一个完整的逐帧模型上增加一些提取时序信息的操作，因此不能减少计算量。后者（如Clockwork Net、Deep Feature Flow等工作）通过重用历史帧的特征来加速计算，这类方法可以减少视频整体计算量，然而忽略了延迟方面的因素。对比这类方法的延迟和精度，可以看出这类方法很难同时实现低延迟和高精度。我们的工作则立足降低每帧平均计算量的同时，实现分割的高精度，降低算法的最大延迟。

### 算法

本文算法使用视频分割中经典的基于关键帧调度的模式来有效平衡计算量和精度。具体来说，如果当前处理帧为关键帧，则使用整个分割网络来获得语义分割的标签，如图2左部分所示；如果当前帧不为关键帧，则变换分割网络高层历史帧特征为当前帧高层特征，再使用分割网络的语义分类操作获得当前帧的语义标签，如图2右部分所示。关键帧的选择和特征跨帧传播两个操作均基于同样的网络低层特征，具体操作在之后章节详述。在划分分割网络结构时，算法尽量保证低层网络的运行时间远小于高层网络，（如图2所示）低层网络耗时61ms，而高层网络耗时300ms。这样考虑的出发点在于：

* 因低层网络的计算代价很小，算法可以基于低层网络提取的特征，增加少部分额外的计算来完成关键帧选择和特征跨帧传播；
* 当前帧的低层特征同样包含当前帧的信息，可以互补来自不同时间的传播特征；
* 所有的操作均复用了逐帧模型的结构，算法整体模型更加简洁。

![&#x56FE;2](../../.gitbook/assets2/image%20%2864%29.png)

#### 1. 特征传播

特征传播关注如何从历史帧传播高层特征到当前帧，降低模型总体计算量，先前的变换方法主要分为两类：

* 基于图像或底层特征获取的光流信息，跨帧传播不同帧的语义分类特征。这类方法虽然有效，但是计算光流往往代价太大，而获得当前帧的语义标签并不需要严格的点到点映射。
* 平移不变性卷积。这种操作在每个位置均使用相同的卷积核来映射特征，因此不能适应不同位置的内容变化。

本文设计了一个位置相关的卷积操作来进行跨帧特征传播。它的计算量相对较低，同时又能适应不同位置的特征进行自适应传播。不同位置的卷积核参数通过一个小的网络回归学习获得（如图2中weight predictor所示），其能很好的适应不同空间位置内容的变化。整体特征传播模块（包含当前帧低层网络、卷积核预测和空间变化卷积）包含两大优势：

* 总体计算量相较高层网络部分计算量大为减小，因而可以快速的获得当前帧的语义标签；
* 可以很好的保持视频邻近帧的抖动或者其他快速变化，实验结果表明这种卷积操作融合方法能够有效的提升7% mIOU的精度。

#### 2. 关键帧选择算法

现有的关键帧调度算法分为固定长度调度和基于阈值调度两种方案，前者不能适应不同视频帧之间内容的变化，后者则通过计算当前帧高层特征和历史帧高层特征之间的差值，通过设定一个阈值来决定是否是否选择当前帧为关键帧，这种方法能一定程度的适应不同帧之间的内容变化，但是特征的差值容易波动，较难设定一个统一的阈值。本文算法使用当前帧语义标签和前一个关键帧语义标签的差异值来作为视频内容变化程度的判断依据。若当前帧距上一个关键帧越远，则语义标签的差值就越大。当差值超过某个阈值的时候，则选择该帧作为关键帧。本文在Cityscapes和Camvid两个数据集上发现低层特征和语义标签的变化值有很大的关联，因而利用历史帧低层特征和当前帧低层特征到一个回归器来回归该差异值。

#### 3. 网络框架

如下图所示

![](../../.gitbook/assets2/image%20%2856%29.png)

### 实验结果

该系统极大的减少了整体耗时，其中判断关键帧操作耗时仅20ms，跨帧特征传播仅需38ms，而高层网络计算高层特征则需要299ms。通过这种方式，整个系统可以明显的降低系统的平均每帧计算量，自适应调度策略和自适应特征传播方法可以把每帧平均计算时间由360ms减为171ms，精度仅损失3.4% mIOU。

同时本文设计了一种低延迟的调度策略进一步减少整体系统的延迟，适用于自动驾驶等需要及时响应的系统。具体而言，当前帧被判断为关键帧时，低延迟调度策略仍然从历史帧传播特征到当前帧并将其缓存为当前帧高层特征，同时启用一个后台线程来计算当前帧高层特征（如果直接运行高层网络部分会造成299ms的延迟），一旦计算完成就取代缓存的高层特征。实验结果表明，这种低延迟的调度策略能够将延迟由360ms降为119ms，同时只损失较小的分割精度（由78.84%降为75.89%）。

## 2. What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets

Spotlight

### 简介

很多视频理解模型都利用了视频的运动信息（Motion）。但运动信息对于视频理解任务来说到底有多重要呢？重要性能不能量化呢？本文提出了一些方法来“去掉”视频中的运动信息，并通过考察一个视频分类模型在去掉视频运动信息前后的分类精度差异来量化运动信息的重要性。本文得出的结论是，运动信息对于视频理解任务的重要性没有想象中那么高；在特定的数据集上，去除运动信息仅使分类精度下降6%。

### 算法

如何去掉视频中的运动信息？最简单的方法就是挑一帧出来，重复这一帧获得一个和原来一样长的视频。完全不动了不就没有运动信息了。但本文指出，这样做会使数据在时域上的分布产生较大的偏差，所以不能得到可靠的结果。因此，本文先提出了一个用少量帧生成完整视频的方法，称这种方法能缓解上述的偏差。

#### 1. Class-Agnostic Temporal Generator

如果所示，模型从视频里选取一些帧，经过生成器得到一个“脑补”的视频；脑补的视频和原版视频分别经过一个视频识别模型（如果想考察运动信息在视频分类任务上的影响，这里就是一个视频分类模型），用各自产生的特征图的距离作为loss，就能够训练生成器。这样，生成器就能够学习“如何通过少量帧生成一个（对某一个模型来说）和原视频尽量像的脑补视频。选帧的操作可以看作丢弃了部分运动信息；而经过生成器则纠正了时域上的分布偏差。

![](../../.gitbook/assets2/image%20%2836%29.png)

#### 2. Motion-Invariant Frame Selector

有了脑补视频的方法，还差一个选帧的方法。如果选取的是不清楚的帧，那去掉的就不光是运动信息了。本文采用的方法是，先将每一帧分别挑出来，用生成器生成对应的视频，然后用视频分类模型得到各类别分数，最后选择“最高分数的类别分数最高”的那一帧，也就是说，用这一帧生成的视频能使分类器给出最“自信”的估计。

### 实验结果

主要实验是在UCF101和Kinetics数据集上做的，都是视频分类数据集；原视频是16帧，为了不同程度地去掉运动信息，分别从中选取1-16帧，生成脑补视频，对比分类精度。如下图所示，红线是原视频的结果，绿线是用本文方法选帧后生成的视频的结果。蓝线是均匀采样、重复单帧得到的视频的结果。在完全去掉运动信息时（横坐标为1），本文提出的方法（绿线）比naive的方法（蓝线）明显更接近原视频的精度，针对“运动信息的重要性估计”给出了全新的边界。

![](../../.gitbook/assets2/image%20%2843%29.png)



