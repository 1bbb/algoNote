# Image Recognition

## 1. Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination

非参数化方法实现的极端无监督特征学习，Spotlight

### 简介

在标注数据集上训练过的神经网络分类器无须人为干预就可以在各个类别间捕捉明显的视觉相似性。我们研究了这一行为是否可以扩展到传统的监督学习领域之外：我们是否可以仅通过获取可区分单独实例的特征来学习一个可以很好捕捉实例间而非类间明显相似性的特征表示？我们将该思路看做实例级的非参数化分类问题，并使用噪声对比估计来解决大量实例类带来的计算挑战。我们的实验结果表明，在无监督学习条件下，我们的算法性能远超 ImageNet 分类问题上最先进的无监督算法。若有更多的训练数据和更好的网络架构，我们的算法会持续提高测试结果。通过微调学习到的特征，我们进一步获得了半监督学习和物体检测任务的有竞争力的结果。我们的非参数化模型非常紧凑：每张图片有 128 个特征，我们的方法存储一百万张图像仅需 600MB，从而在运行时实现快速最近邻检索。

### 背景

如图 1 所示，包含猎豹（leopard）的图像被识别成美洲豹（jaguar）的概率比识别成书柜（bookcase）高很多。这一观察表明，经典的判别式学习方法在没有干预时可以自动发现语义类别之间的表面（明显的）相似性。换句话说，明显的相似性不是来自语义注释，而是来自图像本身。

![&#x56FE;1](../../.gitbook/assets2/image%20%2850%29.png)

这样就产生了一个问题问题：我们是否可以通过纯粹的判别学习来学到反映实例间表面相似性的度量？图像本身具有鲜明的特征，并且每幅图像与相同语义类别中的其他图像都可能有很大差异 。如果我们在没有语义信息的情况下学习区分单独实例，那么我们最终可能会得到一个可以捕获实例间的表面相似性的特征表示，就像类监督学习在类别间仍然保留表面相似性那样。

无监督学习作为实例级别的判别形式在技术上也引人入胜，因为它可以受益于监督学习判别网络的最新进展，例如，新的网络架构。然而，现在我们还面临着一个重大挑战，即现在「类别」的数量就是整个训练集的大小。对于 ImageNet 来说，「类别」将是 120 万而不是 1000 个类。简单将 softmax 扩展到更多的类是不可行的。

我们通过使用噪声对比估计（NCE）逼近的 softmax 分布并采用近端正则化方法以稳定训练过程来解决这个挑战。为了评估无监督学习的有效性，过去的工作依赖于线性分类器（例如SVM），在测试时将学习到的特征与类别信息结合以便进行分类。但是，我们不清楚未知的测试任务为什么可以将训练学习到的特征线性分离。

我们提倡在训练和测试时都采用非参数化方法，将实例级别的分类看作度量学习问题，其中实例之间的距离（相似度）是以非参数方式直接从特征中计算得到的。也就是说，每个实例的特征都存储在离散的内存块中，而不是网络中的权重。

### 算法

算法框架如图2所示

![&#x56FE;2](../../.gitbook/assets2/image%20%2818%29.png)

#### 1. Softmax选择

**Parametric Softmax**：对于属于不同的类的n张图片，对网络输入图片集合 $$\{x_i\}$$ ，得到输出的features$$\{v_i\} = \{f_{\theta}(x_i)\}$$ 。在传统的parametric softmax下，学习每个class factor $$w_i$$ ，使用

$$
P(i|\bm{v}) = \frac{\exp(\bm{w_i}^T \bm{v})}{\sum_{j=1}^n\exp(\bm{w_j}^T \bm{v})}
$$

来计算 $$w_i$$ 属于i类概率相关。这一方法的缺点是只能作为每个class的prototype，阻止了实例间的显式比较。![](file:////Users/irsisyphus/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/13E15D6A-E48A-CC45-A7E6-7319F6BF2842.png)

**Non-param Softmax**：用无监督的方法，把数据库里n张图片的每一张都看成一类、一个实例。我们可以直接选用这n类的feature $$v_i$$ 来代替权重 $$w_i$$ ，控制 $$||v|| = 1$$ ，并引入温度参数 $$\tau$$ 来控制聚拢程度

$$
P(i|\bm{v}) = \frac{\exp(\bm{v_i}^T \bm{v}/\tau)}{\sum_{j=1}^n\exp(\bm{v_j}^T \bm{v}/\tau)}
$$

作为non-param softmax。最终的优化目标是这n张图片的的联合概率

$$
J(\theta) = -\sum_{i=1}^n \log{P(i|f_{\theta}(x_i))}
$$

这么做好处有二，一是避免计算权重增加计算复杂度；二是不受监督学习的限制，可以自然普适到测试集合。然而当n很大时，这种计算耗时长。尤其在本论文里，n等于图片个数，很容易达到百万级别。

**Noise-Contrastive Estimation \(NCE\)**：为了解决计算复杂度，本文采用NCE来替代non-param softmax。NCE的核心概念是二分data samples和noise samples。对于每个v被分到i的概率P和正则项Z

$$
P(i|\bm{v}) = \frac{\exp(\bm{v}^T \bm{f_i}/\tau)}{Z_i}, Z_i = \sum_{j=1}^n\exp(\bm{v_j}^T \bm{f_i}/\tau)
$$

我们引入均匀分布的噪声 $$P_n = $$ ，并假设它的出现概率是data的m倍。那么v属于i的后验分布为

$$
h(i|\bm{v}) :=P(D=1|i, \bm{v}) = \frac{P(i|\bm{v})}{P(i|\bm{v}) + mP_n(i)}
$$

此时loss函数变为

$$
J_{NCE}(\theta) = -\mathbb{E}_{P_d}[\log{h(i|\bm{v})}]  -m\mathbb{E}_{P_n}[\log{(1-h(i|\bm{v}'))}]
$$

其中 $$v'$$ 取自其他图像的feature，随机按照噪声分布抽取图像。但是计算正则项Z的耗时很大，我们采用蒙特卡洛模拟

$$
Z \approx Z_i \approx n \mathbb{E}_j[\exp(\bm{v_j}^T \bm{f_i}/\tau)] = \frac{n}{m}\sum_{k=1}^m \exp(\bm{v_{jk}}^T \bm{f_i}/\tau)
$$

其中 $$\{j_k\}$$ 是随机选取的indices。经验表明，最初的batches获得的估计结果已经足够好了。NCE方法将对每个sample的处理时间由O\(n\)降低到了O\(1\)。

#### 2. Memory Bank

在memory bank里，$$v_i$$ 代表每个分类的feature（即每个图片的feature，因为我们把一张作为一个instance、一个class）。对每个迭代输入 $$x_i$$ 输出的feature $$f_i = f_{\theta}(x_i)$$ ，经过BP更新 $$\theta$$ 和 $$f_i$$ 后，把 $$f_i$$ 作为新的$$v_i$$。一开始所有的都设置成随机单位向量。因为每张图片有 128 个特征，存储一百万张图像大概仅需 600MB。

#### 3. Loss优化 - Proximal Regularization

由于每一类只有一个data，训练过程震荡很大。我们使用了一个近似优化的方法来平缓训练过程。在每个迭代 $$t$$，对于每个data sample $$v_t$$，记录 $$v_{t-1}$$。则NCE Loss变为

$$
J_{NCE}(\theta) = -\mathbb{E}_{P_d}[\log{h(i|\bm{v}^{(t-1)})} - \lambda||\bm{v}^{(t)} - \bm{v}^{(t-1)}||_2]  - m\mathbb{E}_{P_n}[\log{(1-h(i|\bm{v}'^{(t-1)}))}]
$$

#### 4. Eval方法 - Weighted k-Nearest Neighbor Classifier

对于每个测试例$$\tilde{v}$$，计算它和每一个memory bank里面$$v_i$$的cosine距离，选出k个邻居。然后在给定的超参数下，对邻居的label进行加权投票，获得测试例的label。![](file:////Users/irsisyphus/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/8369BDCE-CE0E-854E-A03B-C0C5F6F082CA.png)

### 实验结果

实验结果表明，在无监督领域，该方法在图像分类方面远超过最先进的方法。具体地，在 ImageNet 1K上的 top-1 准确率为 46.5％，Places 205为 41.6％，并且图像在 softmax 层输出中的预测值排第二的响应类更可能与真实类有视觉关联。若有更多的训练数据和更好的网络架构，该算法会持续提高测试结果。通过微调学习到的特征，可进一步获得半监督学习和物体检测任务的具竞争性的结果。最后，该非参数化模型非常紧凑：每张图片有 128 个特征，存储一百万张图像仅需 600MB，从而在运行时实现快速最近邻检索。

## 2. Iterative Visual Reasoning Beyond Convolutions

超越卷积的视觉推理框架，CMU & Google，Spotlight

### 简介

我们提出了一种新的迭代视觉推理框架。该框架超越了目前只具备卷积堆栈推理能力的识别系统。该框架由两个核心模块组成：一个是局部模块，使用空间记忆以并行更新的方式存储以前的信念；另一个是全局图形推理模块。我们的图模块由三个部分组成：（1） 知识图，在该图中，我们将类表示为节点并建立边来编码它们之间不同类型的语义关系；（2）当前图像的区域图，在该图中，节点代表图像中的区域，边代表这些区域之间的空间关系；（3） 将区域分配给类的分配图。局部模块和全局模块迭代地转出并相互交叉馈送预测，以便细化预测。通过将两个模块中的最佳部分与注意机制相结合来进行最终预测。与普通 ConvNets 相比，我们的框架性能显著增强，例如，按每级平均精度衡量，我们可以在 ADE 上实现 8.4 % 的绝对改进。分析还表明，该框架对缺失区域具有较强的推理能力。

### 背景

我们的目标是建立一个系统，该系统不仅可以提取、利用卷积特征的层次结构，还可以通过空间和语义关系来改进其判断。图 1 左上角是一个空间推理的实例：如果一行中四分之三的区域是「窗户」，那么第四个区域也可能是「窗户」。语义推理的一个实例（右下图）是识别「校车」，即使图中可见的校车很少或基本看不见——仅仅给出了「公交车」的实例及公交车与校车之间的联系。最后，空间-语义推理可以解释为：识别道路上的「汽车」应有助于识别「汽车」内的「人」。

![&#x56FE;1](../../.gitbook/assets2/image%20%289%29.png)

利用关系进行推理的一个关键方法是迭代地进行预测。最近，已经有人尝试通过自顶向下的模块或使用外显记忆来整合这种推理。在使用自顶向下的模块时，具有基于类的信息的高层次特征可以与低层次特征结合使用，以提高识别性能。另一种架构是使用外显记忆。例如，Chen 和 Gupta 展示了序贯对象检测，其中空间存储器用于存储先前检测到的对象，利用 ConvNets 的能力来提取有利于后续检测的密集语境模式。

然而，这些方法存在两个问题：（1） 两种方法都使用卷积堆栈来执行局部像素级推理，该方式可能不具备允许更远区域直接传递信息的全局推理能力（2）更重要的是，这两种方法都假定训练数据中有足够的关系实例供模型从头开始学习，但是随着类数量的增加，关系呈指数级增长，数据可能出现短缺。许多语义推理需要从很少或近乎为零的实例中学习。因此，我们需要设法利用额外的结构化信息进行视觉推理。

### 算法

本研究提出了一个空间推理和语义推理的通用框架。与目前仅依赖卷积的方法不同，我们的框架还可以从知识库形式的结构化信息中学习视觉识别。我们算法的核心由两个模块组成，一个是基于空间记忆的局部模块，利用 ConvNets 进行像素级推理。我们借助并行内存更新来大幅度提高效率。另一个是基于图结构的全局模块，用于在局部区域之外进行推理。

该模块由三个部分组成：

* 知识图，在该图中，我们将类表示为节点并建立边来编码它们之间不同类型的语义关系；
* 当前图像的区域图，在该图中，节点代表图像中的区域，边代表这些区域之间的空间关系；
* 将区域分配给类的分配图。

利用这种结构，我们开发了一个推理模块，专门用来传递该图中的信息。局部模块和全局模块两者迭代地转出并相互交叉馈送预测，以便细化预测。注意，局部推理和全局推理不是孤立的：良好的图像理解通常需要平衡先验学习的背景知识和图像特定观察之间的关系。因此，我们的完整管道通过「关注」机制连接了两个模块的力量，从而使模型在进行最终预测时能够依赖最相关的特征。整体架构如图2所示。

![&#x56FE;2](../../.gitbook/assets2/image%20%2858%29.png)

#### 1. LOCAL - Reasoning with Convolutions：

我们第一个建立的区块——局部模块，由文章启发。在较高层面来看，这一想法是用空间记忆S来存储检测到的目标物，存储地点就在目标物被检测到时所在的位置。S是三维张量，高度H和宽度W为图像缩减后的尺寸（1/16），深度D（=512）将记忆的每个单元c作为向量，存储当前位置可能有用的信息。

S通过高层和中层特征来更新，高层存储有关估计的类别标签信息。但是，只知道类别并不理想，更多关于形状、姿态等的细节信息在其他物体检测时大有用处。例如，预先知道一个“人”打网球的姿势，会对识别“网球拍”大有帮助。本文中，我们在softmax激活前用logits $$f$$ ，结合底部卷积层h的特征图来补给记忆。

给定一个待更新的图像区域r，我们要先从底层提取相应特征，用双线性插值将其调整为预定大小（7\*7）的方阵h。因为高层特征f是覆盖整个区域的向量，所以我们将其附加在所有位置（49个）。用两个1\*1的卷积核来提取特征并为r生成输入特征。记忆S中的相同区域也提取出来，调整为7\*7，标注为。这一步后，我们用卷积门递归单元（GRU）来写出记忆：

$$
s_r' = u \circ s_r + (1-u) \circ \sigma(W_f f_r + W_s(z \circ s_r) + b)
$$

其中，$$s_r'$$ 是 r 更新后的记忆，u 是更新后的门，z 是重置门，$$W_f, W_s$$ 和 b 分别是卷积的权重和偏置，$$\circ$$是 entry-wise 矩阵内积，$$\sigma$$为激活函数。更新后，$$s_r'$$ 通过再提取和尺寸调整放回到S。

文章同时采用了parallel update。前期的研究工作将序列更新到记忆；但是，序列推断低效且 GPU 密集，这限制其每张图像只能有 10 个输出。本文提出用并行更新区域作为近似。在交叠情况下，一个单元可以由不同区域覆盖多次。将区域放回至S时，我们也计算权值矩阵 $$\Gamma$$ ，矩阵中，对于每一个入口 $$\gamma_{r, c} \in [0, 1]$$ 记录了区域 r 对记忆单元 c 的贡献率：1 表示单元被区域完全覆盖，0 表示没有覆盖。更新后的单元的终值是所有区域的加权平均。

实际的推理模块为一个ConvNet $$C$$ ，由三个 3\*3 的卷积核和两个 4096D 的全连接层组成，以 S 为输入，在接受域的局地窗间建立连接关系来进行预测。因为图像的二维结构和位置信息保存在 S 中，所以，这样的结构对于空间推理关系非常有用。

#### 2. GLOBAL - Beyond Convolutions

第二个模块采用的是对全局推理进行卷积操作而不是针对局部区域进行的。这里的“global”全局有两个含义。首先从空间角度来讲，我们希望让更远的区域也可以彼此直接进行信息交流，而不受推理模块C的接受域的限制。其次从语义层面来说，我们希望可以充分的利用视觉知识库，它可以提供跨图像的全局真实的类间关系\( 比如常识类信息 \)。为了实现以上两个层面的推理，我们构造了一个图 $$G=(N,E)$$ 中N和E分别为节点集和边集。在N中定义了两种类型的节点：区域R的区域节点$$N_r$$和类C的类节点$$N_c$$。

对于E，在节点之间定义三组边。首先，对于$$N_r$$来说，我们使用空间图来编码区域$$E_{r \rightarrow r}$$之间的空间关系，设计多种类型的边来表征相对位置。我们从诸如“左/右““上/下”之类的基本关系开始，并且通过测量两者之间的像素级距离来定义边缘权重。需要说明的是，我们并不是直接使用原始距离x，而是使用核 $$\kappa(x) = \exp(-x/\Delta)$$ \(其中 $$\Delta)$$ 代表带宽\) 将其归一化为 \[0, 1\]，这样可以使得越接近的区域相关程度越高。然后直接在图的邻接矩阵中使用边缘权重。此外，为了能够有效的解决区域重叠时的分类问题，我们也把用于编码覆盖模式的边 \( 例如IoU intersection over union\) 包含在了其中。

第二组边是位于区域和类之间的集合，即决定一个区域是否属于某一类。这些边缘的作用是，将信息从一个区域传播到另一个类别$$e_{r \rightarrow c}$$或从一个类别反向传播到另一个区域$$e_{c \rightarrow r}$$。我们采用soft-max值p来定义连接到所有类别的边缘权重，而不是仅仅计算连接到可能性最高的那一类。这样做是希望它能提供更多的信息，并且提高对错误分类的鲁棒性。

第三组边是使用知识库中的语义关系构造类间$$\mathcal{E}_{c \rightarrow c}$$关系的集合。同样，这里可以包括多种类型的边。经典的例子有“is-kind-of”\( 例如 “蛋糕”和“食物”\)、“is-part-of”\(例如“车轮”和“汽车”\)、“相似性”\( 例如“豹”和“猎豹”\)，其中许多都是普遍正确的，因此可以被认为是常识。这种常识可以是人工列出的或自动收集的。有趣的是，即使是这些以外的关系\(例如“行为”，“介词”\)也可以帮助识别。以“人骑自行车”为例，这显然是一种更加具体形象的关系。然而，由于对“人”和“自行车”这两类的识别准确度不够高，那么从两者之间的“骑行”的关系以及他们的空间位置出发，则可以帮助我们对这一区域进行更好的推理。为了研究这两种情况，我们在本文中尝试了两种知识图谱：一种是内部创建的，大部分是常识边，另一种则是大规模积累的更多类型的关系。有关我们实验中所使用的图，请参阅4.1，那里会对这一部分工作进行更详情的介绍。

具体的操作方法如图3所示。这里，四个结点连接两个类型的边，每个结点表示一个输入特征向量$$m_i$$（集合为M）。权值矩阵 $$W_j$$ 学习为边类型j来转换输入量。之后连接矩阵 $$A_j$$ 用来向关联结点传递信息。最后，通过累计所有的边类型并使用激活函数生成输出G。

![&#x56FE;3](../../.gitbook/assets2/image%20%2871%29.png)

具体地分析一下这个基于图的推理模块r：作为图的输入，我们使用$$M_r \in \mathbf{R}^{R\times D}$$来表示来自所有区域节点$$N_r$$组合的特征，其中D \( = 512 \)是特征信道的数目。为了方便表示，对于每个类节点$$N_c$$我们选择现成的字向量作为表示，记为$$M_c \in \mathbf{R}^{C\times D}$$。需要注意的是，因为我们的最终目标是为了能够更好地识别区域，所以所有的类节点都只是中间表示，为的是能够更好地表示区域。基于此，我们设计了两种推理路径来学习输出特征$$G_r$$：

* 仅包含区域节点的空间路径：$$G_r^{spatial} = \sum_{e \in \mathcal{E}_{r \rightarrow r}} A_e M_r W_e$$
*  通过类节点的语义路径：$$G_r^{semantic} = \sum_{e \in \mathcal{E}_{c \rightarrow c}} A_e \sigma(A_{e_r \rightarrow c} M_r W_{e_r \rightarrow c} + M_c W_c)W_e$$
* 最终合并的输出为：$$G_r = \sigma(G_r^{spatial}+\sigma(A_{e_c \rightarrow r}G_r^{semantic}W_{e_c \rightarrow r}))$$

具体方法是，首先将语义信息传播回区域，然后使用非线性激活，如图4 。

![&#x56FE;4](../../.gitbook/assets2/image%20%2894%29.png)

与卷积滤波器一样，上述路径也可以堆叠，其中可以经过另一组图形操作输出，从而允许框架执行具有更深特征的联合空间语义推理。在输出被反馈用以进行预测之前，我们在R中使用了具有残差连接的三个操作栈。

#### 3. Iterative Reasoning

推理的一个关键要素是迭代地建立估计。但是信息是如何从一个迭代传递到另一个迭代的呢？我们的答案是显式内存，它存储之前迭代的所有历史记录。本地模块使用空间存储器S，全局模块使用另一无空间结构的存储器M。对于第i次迭代，$$S_i$$之后是卷积推理模块C，以生成每个区域的新预测$$f_i^l$$。类似地，全局模块还给出来自R的新预测$$f_I^j$$。这些作为高级特征的新的预测可以用于对存储器$$S_{i+1}$$和$$M_{i+1}$$进行更新。新的记忆将带来$$f_{i+1}$$轮更新，以此类推。

虽然可以单独进行局部和全局推理，但这两个模块协同工作时的效果是最好。因此，我们希望在生成预测时加入两个模块的结果。为此，我们引入了cross-feed 连接。在完成推理后，将局部特征和全局特征连接在一起，利用GRU更新$$S_{i+1}$$和$$M_{i+1}$$两个存储单元，这样，可使空间存储单元能够从空间和语义关系的全局知识中受益，并且图能够更好地理解局部区域的布局。

#### 4. Attention

模型产生attention来表示当前预测与来自其它迭代或模块的预测的相对置信度，从而融合输出使用attention的所有预测的加权版本。如果模型迭代展开I次，则对于输出宽度为N = 2I + 1 \(包括I个局部、I个全局和1个初始的ConvNet网络\) 的$$f_n$$，使用attention $$a_n$$，输出f：

$$
f = \sum_n w_n f_n, w_n = \frac{\exp(-a_n)}{\sum_{n'}\exp(-a_{n'})}
$$

需要注意的是，这里$$f_n$$是softmax之前的logits，被激活以后产生$$p_n$$。Attention的引入使模型能够智能地从不同的模块和迭代中选择最可行的预测。

#### 5. Loss functions

最后，对整个框架进行端到端的训练，总损失函数包括普通ConvNet损失、本地模块损耗、全局模块损耗、以及包含attention的最终预测损失。因为希望推理模块更多地关注较难的示例，模型根据以前迭代的预测，简单地对损失中的示例进行重新加权。具体的，对于迭代$$I \ge 1$$时的区域r，两个模块的交叉熵损失计算为：

$$
\mathcal{L}_i(r) = \frac{\max{(1. - p_{i-1}(r), \beta)}}{\sum_{r'}\max{(1. - p_{i-1}(r'), \beta)}} \log(p_i(r))
$$

其中$$p_i(r)$$是ground-truth类的softmax输出，$$\beta \in [0, 1]$$控制权重分布的熵：为1时是均匀分布，为0是熵最小，实验中取0.5。$$p_{i-1}(r)$$用作没有反向传播的特征。对于本地和全局来讲，$$p_0(r)$$是来自普通卷积神经网络（ConvNet）的输出。

### 实验结果

本文的框架展示了大大超越普通 ConvNets 的性能。例如，框架可以在 ADE上实现 8.4 % 的绝对提升，这是按每级平均精度衡量的，而通过加深网络仅可以提高约 1 %。

但是，如果一个物体没有在训练数据里和其他物体出现过，预测含有这个物体的新场景时可能失败。

## 3. Context Encoding for Semantic Segmentation

Oral

### 简介

本文提出一种与类别预测相关的网络结构，使得在一定程度上降低了分割任务的难度，同时提高了小物体的分割精度。本文提出上下文语义编码模块与类别预测模块，在某种程度上解决或减轻了分割问题中类间样本不均衡的问题，而这类问题在以像素为度量的损失函数中是非常常见的。

### 算法

#### 1. Semantic Encoding Loss \(SE-loss\)

在标准的语义分割训练过程中，网络是从分离孤立的像素点中学习到的（对于每个输入的图像与标签，使用像素级别的交叉熵损失函数）。网络在学习全局上下文信息的时候可能会有一定的困难。为了规范化上下文编码模块的训练过程，这里引入一种上下文语义编码损失函数，即用来预测场景中出现的类别，使用多类的 sigmoid cross entropy loss。与像素级别的交叉熵损失函数不同，SE-loss只考虑类别信息，对每类中像素的数量不关心，这样每类都是平等的，不存在类间样本数量不均衡的问题。因此，我们发现在使用这个损失函数后，好多小物体的分割效果变好了。

【疑问：这个思路是，对于语义feature maps做attention，但是对语义类别做均等提取，使得能在对信息有attention的基础上避免漏小物体。但是是否可以把这二者合一呢，比如meta learning 训练一个比重，或者引入GAN？】

#### 2. Context Encoding Network \(EncNet\)

网络结构如图2所示：

![&#x56FE;2](../../.gitbook/assets2/image%20%283%29.png)

从图中可以看出，这个网络结构中，对前面网络提取出的丰富的特征使用全连接层FC进行编码，其中一个编码分支直接送给SE-loss，进行场景中出现类别的预测；另一个分支对每个类别预测加权的尺度因子，然后这个加权的尺度因子对前面提出的每个通道的类别进行加权，然后再进行后面的上采样，最终计算loss。CNN网络采用dialated network的方法进行训练，CEM模块构建在网络的最后预测层的前一层之上。

### 实验结果

目前为同类模型里最高，在PASCAL-Context达到 51.7% mIoU，在PASCAL VOC 2012达到85.9% mIoU，在ADE20K test set得分0.5567（超过COCO 2017最好模型）。用仅仅14层即可在CIFAR-10达到3.45%错误率。

## 4. An Analysis of Scale Invariance in Object Detection – SNIP

Oral

### 简介

我们发现，CNN对于尺度（scale）的变化是不鲁棒的，由此我们提出在图像金字塔上的同一尺度训练检测器。既然小物体在小尺度难检测、大物体在大尺度难检测，我们提出了一个新的训练方法Scale Normalization for Image Pyramids \(SNIP\)：SNIP利用一个以图片大小为参的函数，选择性地BP不同大小的物体。最终网络的结果在COCO 2017上获得最佳学生结果（48.3% mAP）。

### 背景

在图像分类这个问题上，ImageNet top5错误率可以低至 2%。但是在目标检测这个领域，即使在 50%的重合率上，最好的检测器在 COCO 数据集只有 62%的正确率。目标在图像中的大尺度变化范围，尤其是小目标检测的挑战是目标检测诸多难点中显著的一个。比如在COCO 中，大部分目标在图像中小于 1% 的面积，且最小和最大10%的目标在图像中的占面积比分别是是 0.024 and 0.472（导致尺度相差近20倍）。更糟糕的是我们的检测器一般都是在图像分类数据库上预训练，再进行微调得到的。但是检测和分类中的目标尺度差异很大，这有导致了一个大的domain-shift 。

前人提出了解决这个多尺度问题的一些方法：使用多尺度特征做检测；dilated/deformable convolution用于增加大目标的感受野；对不同分辨率下做独立的检测； context；多尺度训练；多尺度检测等等。虽然这些思路提升了目标检测的性能的，但是下面的问题仍然没有被解决

* 放大图像对于目标检测的性能至关重要吗？检测数据库中图像的尺寸大多是 480x640，为什么在实际中人们常常将图像放大到 800x1200？我们可以在ImageNet预训练时对小尺寸图像可以采用小的strides，然后在检测数据库中微调来提升小目标的检测吗？
* 当我们对预训练的分类模型在检测上进行微调得到一个检测器时，在对输入图像尺寸归一化后，训练的目标尺度需要限定在一个小的范围吗（from 64x64 to 256x256）？或者是在放大输入图像后，需要训练所有尺度的目标吗\(from 16x16 to 800x1000, in the case of COCO\)？

Scale space theory 提倡学习尺度不变的特征来解决这个多度问题。当前CNN模型中的 deeper layers 具有较大的 strides \(32 pixels\)，这导致了输入图像对应了一个非常粗糙的representation，这时的小目标检测很难做。为了解决这个问题，有人使用 dilated/atrous convolutions 来增加 feature map 的尺寸。Dilated/deformable 卷积也保留了预训练网络的参数和感受野，所以对大目标检测性能没有下降。 在训练时将图像放大1.5到2倍，在 预测时输入图像放大 4 倍 也是一个常用的增加 feature map 的尺寸的方法。因为考虑输入图像的网络层 feature map 的尺寸大，所以有人将各个网络层的 feature map 结合起来做检测，也有分别在这些 feature map 上做检测。 方法如 FPN，Mask-RCNN，RetinaNet 采用了pyramidal representation，将多网络层 feature map 结合起来检测。但是当目标尺寸为 25x25 像素时，即使在训练时放大 2倍，目标也只有 50x50 像素。通常在图像分类预训练的图像尺寸为 224x224，这时就无法检测这么小的物体了。所以模型通常将shallow layers 的特征结合起来做检测，但它们对于小目标检测效果不好。最近也有一些方法使用个金字塔方式检测人脸，对每个尺度的目标梯度BP，对于不同尺寸目标在分类层使用不同的滤波器。这个方法只适用于人脸检测，不适用于广义的目标检测：因为在目标检测里，每一类的训练数量是有限的，且在外表存在很大的差异性；而人脸的相似度很高。为此我们提出了一个选择性的BP算法，针对每个尺寸，使用相同的滤波器。本文使用Deformable-RFCN作为基准检测器，它在公开代码的模型中，获得了COCO上最好的成绩。

#### 测试1 - 不同尺度下的图片分类

首先我们来分析一下domain shift的影响，这是由训练和检测图像尺寸不一样导致的：一般训练的图像尺寸是 800x1200 ，inference 的图像尺寸是 1400x2000（为了检测小目标）。

首先我们对 ImageNet图像进行降采样，获得不同尺寸的图像48x48, 64x64, 80x80, 96x96 and 128x128，然后再归一化到 224x224，用这个尺寸训练一个 CNN网络。我们得到的结论是：随着训练集和测试集的像素差别增大，模型的表现也会变差。

基于上述观察结果，一个提升小目标检测性能的方法是在ImageNet上预训练不同stride的网络，测试结果也确实有所提升（CNN-S）；换句话说，用不同的网络架构预训练，对于低分辨率的物体进行检测时用低分辨率的图像。另一个提升小目标检测性能的方法是在上采样的低分辨率图像上fine tune，取得了比前一个方法更好的结果（CNN-B-FT）。最终结论是放大图像+用在高画质下预训练然后在低画质下FT的模型，比专门针对小目标训练一个分类器要好。结果如下图所示，原始模型为CNN-B，结果CNN-B &lt; CNN-S &lt; CNN-B-FT。

#### 测试2 - 图像尺寸、目标尺寸、尺度范围对检测器性能的影响：

首先设定了图像选取方法：

* 多个尺度：
  * $$800_{all}$$：对应的训练图像尺寸为 800x1200
  * $$1400_{all}$$：对应的训练图像尺寸为 1400x2000
* Scale specific detector
  * $$1400_{<80px}$$： 对应在 1400x2000尺寸上训练，将大、中目标去除
* Multi-Scale Training \(MST\)： 在训练时随机对图像在不同的分辨率下取样
* SNIP：只对落入理想尺度的物体训练，其它的物体在BP时忽略

四种方法如图5所示，结论：

* $$1400_{all}$$比$$800_{all}$$效果要好，但是这种提升很小，因为在高分辨率下训练时能更好分类出小目标，但是过于放大中大型物体了。
* $$1400_{<80px}$$效果最差，因为失去了太多的重要训练样本
* MST 性能和$$800_{all}$$相当

![](../../.gitbook/assets2/image%20%2828%29.png)

### 算法

SNIP的框架如图6所示

![&#x56FE;6](../../.gitbook/assets2/image%20%2835%29.png)

### 实验结果

COCO2017 48.3% mAP

## 5. Relation Networks for Object Detection

Oral

### 简介

在视觉领域，我们都知道，建模物体间的关系，有利于目标识别任务。但是在深度学习领，，很少有利用物体关系进行目标识别任务的。本文提出了一种目标关系模块（Object Relation Module），它同时处理一组目标，对目标之间的外观特征关系和位置关系进行建模。该模块的输入输出in-place，不需要额外的监督，因此很容嵌入到已有网络中，作者将目标关系模块应用到区域特征提取后的 FC 层，使目标特征包含物体间的关系信息，增强目标识别能力。作者还将目标关系模块应用到去重阶段，代替传统的 NMS 算法，提高网络识别精度，同时可以使网络进行端到端的训练，如图1所示。实验表明，在目标检测网络的目标识别和去重两个阶段添加目标关系模块，可以提高检测精度，并实现完全的端到端目标检测器

![&#x56FE;1](../../.gitbook/assets2/image%20%282%29.png)

### 算法

#### 1. 目标关系模块

![](../../.gitbook/assets2/image%20%2849%29.png)

![](../../.gitbook/assets2/image%20%2845%29.png)

![](../../.gitbook/assets2/image%20%2878%29.png)

![](../../.gitbook/assets2/image%20%2873%29.png)

![&#x56FE;2](../../.gitbook/assets2/image%20%2829%29.png)

#### 2. 实例检测阶段

Faster R-CNN，FPN，DCN 等目标检测网络，RoI Pooling 层输出的特征都会与2个全连接层相连，然后预测类别得分和边界框回归。作者在每个 FC 层后，加入目标关系模块，保证输入 N 个 proposals 前后，特征维度不发生变化。具体流程如图3\(a\)所示。

#### 3. 去重阶段

作者将去重看作一个二分类问题，其流程如图3\(b\)所示。

![&#x56FE;3](../../.gitbook/assets2/image%20%2872%29.png)

![](../../.gitbook/assets2/image%20%2814%29.png)

![](../../.gitbook/assets2/image%20%2811%29.png)

### 实验结果

使用2fc head和去重都可以提升一定的模型预测结果，但是提升并不是很明显。

【思考：在早期使用物体关系的方法中，通常关系是显式的，比如有人的图片很可能有车，而有船的图片不太可能有车。或者物体的空间信息，比如天空在图片上方，地面在图片下方等。本文将Scaled Dot-Product Attention应用到检测任务中。虽然学到的关系模块并不能显式的表示出物体之间的关系，但是因为在学习时，融入了位置关系和外观特征。实验表明，也的确有效果。那么关系模块学习到的到底是什么呢，尤其是多个关系模块组合时，并不能很清楚说出其中的道理。这个方法是不是也可以应用到其他领域，比如跟踪。】

## 6. Im2Flow: Motion Hallucination from Static Images for Action Recognition

Oral

### 简介

本文提出了Im2Flow框架，通过学习大量视频中的动作先验，来预测静态图片的光流，从而对静态图片中的动作作出判断。这种方法可以获得目前最好的光流预测，且能够极大提升动作识别的准确率。

### 算法

框架如图2所示，用u-net训练一个模型，输入是一个静态的帧，输出的预测的五帧光流信息，模型在youtube数据集上训练。该模型loss分两部分，一部分是将生成的五帧光流信息与youtube数据集的ground-truch对比，另一部分是将生成的光流信息与真实的光流信息送进loss网络（在ucf101上预训练过的resent）计算欧式距离；前者是要求准确，后者是为了更好保留high-level的动作信息。作者也实验了cGAN，发现只能较好的区分样本，而不能像欧式距离的loss那样获得比较精准的pixel-wise比较结果。

![&#x56FE;2](../../.gitbook/assets2/image%20%2877%29.png)

最终将静态图片和预测的五帧光流送入标准two-stream模型即可预测，two-stream模型如下图所示。

![](../../.gitbook/assets2/image%20%2887%29.png)

### 实验结果

在UCF-101数据集上获得state-of-the-art的dense optical flow prediction结果；在UCF-101、HMDB-51和Penn Action数据集上获得较高的静态图片动作预测结果；在static-YUP++和Willow数据集上获得state-of-the-art的静态图片动作预测结果。

