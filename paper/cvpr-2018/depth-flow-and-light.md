# Depth, Flow & Light

## 1. Single View Stereo Matching

单目深度估计技术，商汤科技，Spotlight

### 简介

基于视觉的自动驾驶系统需要基于单目摄像头获取的图像，判断当前车辆与周围车辆、行人和障碍物的距离，距离判断的精度对自动驾驶系统的安全性有着决定性的影响，本论文提出基于单目图像的深度估计算法，大幅度提升基于单目图像深度估计的精度，进一步提升自动驾驶系统的安全性。单目深度估计对比双目深度估计具有方便部署、成本低等优点，在很多领域有着丰富的潜在应用场景，如三维重建、增强现实等。

### 算法

基于单目图像的深度估计算法具有方便部署、计算成本低等优点，受到了学术界和工业界日益增长的关注。现有的单目深度估计方法通常利用单一视角的图像数据作为输入，直接预测图像中每个像素对应的深度值，这种解决方案导致现有方法通常需要大量的深度标注数据，而这类数据通常需要较高的采集成本。近年来的改进思路主要是在训练过程中引入隐式的几何约束，通过几何变换，使用一侧摄像机图像（以下称右图）监督基于另一侧摄像机图像（以下称左图）预测的深度图，从而减少对数据的依赖。但这类方法在测试过程中仍然缺乏显式的几何约束。为了解决上述问题，本文提出单视图双目匹配模型\(Single View Stereo Matching, SVS\)，该模型把单目深度估计分解为两个子过程，视图合成过程和双目匹配过程，其算法框架如图1所示。

![&#x56FE;1](../../.gitbook/assets2/image%20%2837%29.png)

通过这样的分解，使得提出的模型有如下两个优点：

* 极大地减少深度标注数据的依赖；
* 在测试阶段显式地引入几何约束。

基于上述分析，本文方法提出了一种新颖的面向单目深度估计的算法框架，把单目深度估计分解为两个过程，即视图合成过程和双目匹配过程。模型的主要设计思路在于：

* 把双目深度估计模型中有效的几何约束显式地结合到单目深度估计模型中，提高模型的可解释性；
* 减少使用难以采集的真实深度数据，从而扩大模型的适用范围；
* 整个模型以端到端的的方式训练，从而提升深度估计准确性。

模型的视图合成过程由视图合成网络完成，输入一张左图，网络合成该图像对应的右图；而双目匹配过程由双目匹配网络完成，接收左图以及合成的右图，预测出左图每一个像素的视差值，详细的网络结构如图2所示。

![&#x56FE;2](../../.gitbook/assets2/image%20%2885%29.png)

**视图合成网络**：一般情况下，左图中的像素均可以在右图中找到匹配的像素，因此可以首先把左图平移多个不同像素距离，得到多张不同的图片，再使用神经网络预测组合系数，把多张平移后的左图和预测的系数组合得到预测的右图。具体地，视图合成网络基于Deep3D 模型，图2中的上半部分展示了视图合成网络的示意图。输入一张左图，首先主干网络对其提取不同尺度的特征，再经过上采样层把不同尺度的特征统一至同一个尺寸，然后经过累加操作融合成输出特征并预测出概率视差图，最后经过选择模块（selection module）结合概率视差图以及输入的左图，得到预测的右图。本文采用L1损失函数训练这个网络。

**双目匹配网络**：双目匹配需要把左图像素和右图中其对应像素进行匹配，再由匹配的像素差算出左图像素对应的深度，而之前的单目深度估计方法均不能显式引入类似的几何约束。由于深度学习模型的引入，双目匹配算法的性能近年来得到了极大的提升。本文的双目匹配网络基于DispNetC 模型, 该模型目前在KITTI双目匹配数据集上能够达到理想的精度，其网络如图2的下半部分所示，左图以及合成的右图经过几个卷积层之后，得到的特征会经过1D相关操作（correlation）。相关操作被证明在双目匹配深度学习算法中起关键性的作用，基于相关操作，本文方法显式地引入几何约束；其得到的特征图和左图提取到的特征图进行拼接作为编码-解码网络（encoder-decoder network）的输入，并最终预测视差图。该网络的训练也同样使用L1损失函数。

### 实验结果

本文提出的模型仅用少量的深度标注数据就可以在KITTI数据集上超过之前的所有单目深度估计方法，并首次仅靠单目图像数据就超过了双目匹配算法Block Matching的深度估计精度。

## 2. PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume

Nvidia，Oral

### 算法

PWC-Net使用特征金字塔、warping，和cost volume预测光流。以前的模型直接把预测的（上采样过的）光流直接warp到图片帧上，而PWC-Net利用一个可学习的特征金字塔，把预测的（上采样过的）光流warp到图片帧的feature maps上，取得了比以前模型都好的结果。流水线如图3所示。

![](../../.gitbook/assets2/image%20%2839%29.png)

其中Warping Layer用一个网络层替代了直接warp操作；Cost volume layer计算两个输入feature maps的每个像素位置的correlation值，储存成张量，代表匹配度；Optical flow estimator接收这个张量、第一帧的feature maps、和预测的光流，输出最终的光流；Context network为post processing的可选网络。实验结果表明这几个设计都或多或少可以提升最终效果。详细架构见[链接](http://xiaodongyang.org/publications/papers/pwc-supp-cvpr18.pdf)，用DenseNet架构可以提升optical flow estimator的效果。

### 实验结果

PWC-Net在clean pass上不如传统模型，因为它们往往显式考虑edge，但在最终视频里往往不那么明显；而PWC-Net在final pass上却能取得更低的test error，更接近现实（也会获得更高的train error，说明generality更好）。

## 3. Optimal Structured Light a la Carte

### 背景

结构光技术可以用于计算三维场景的深度图。使用结构光计算深度图的系统由一个投影仪和一个摄像机组成，投影仪向目标物体投射一组设计好的图案，摄像机拍摄被投射了图案的目标物体。由于摄影机投射了多组图案，摄像机也会拍摄到多组图案叠加在目标物体上的图像。这样，摄像机每一行的每一个像素位置上都能观测到一组亮度值，每个亮度值对应一个图案。同样地，投影仪每一行的每一个像素位置上也有这样一组亮度值。对于投影仪、摄像机同一行上的像素，可以利用这个亮度值向量的相似度来计算对应关系。有了像素的对应关系，就能计算出场景对应点的深度。

本文解决的问题是：在给定一些参数的情况下（如投影图案的个数等），如何获得最优的投影图案。其核心思想是，隐式地随机生成一些场景（当然没有真的生成，只是生成了含有场景信息的矩阵；这些矩阵决定了投影仪和摄像机像素的“正确的”对应关系），然后迭代优化投影图案使得像素匹配的错误率最小。

### 算法

考虑投影仪和摄像机处于同一水平面（epipolar plane）的像素。摄像机观察到的内容可以由下式计算：

$$
[o_1, \cdots, o_M] = [c_1, \cdots, c_N] \bm{T} + \bm{1} [a_1, \cdots, a_M] + \bm{E}
$$

其中，o是摄像机每个像素位置处观察到的编码向量（即每个结构光图案在这个点处的亮度），c是投影仪每个像素位置处投影的编码向量。T是转移矩阵，其每一列只有一个元素非0，非0元素在0到1之间。a是环境光，E是噪声。在这个模型下，每一个观测位置上的编码向量一定是从投影的一行向量中的一个，再乘上一个系数；所以T决定了每个观测位置“看到”的是哪一个投影的向量。在实际场景下，场景确定，每一行观测-投影位置的对应关系就已经确定；所以也可以说，T代表了场景的信息，可以从T推出投影-观测像素对应关系的“Ground Truth”。

上式描述了结构光投影-观测的一种模型。而实际情况下，T是未知的。如何根据投影内容和观测内容计算每行像素的匹配关系呢？本文使用了一种简单的距离公式：

$$
ZNCC(o_q, c_p) = \frac{o_q - mean(o_q)}{||o_q - mean(o_q)||} \frac{c_p - mean(c_p)}{||c_p - mean(c_p)||}
$$

用这种距离度量观测编码向量和每个投影编码向量的相似性，直接匹配最像的就行了。

本文的目的是给出最优的投影图案，也就是最优化c，使得匹配错误率最小。这里的“前向传播”就是用上述距离公式计算匹配关系，“反向传播”就是用T推算Ground Truth的匹配关系，然后用softmax loss计算误差值，用Adam优化。至于T，a，E等，则全部随机生成；共有数百个，在优化c的过程中固定，作为在每个设定下对场景的“期望”。

### 实验结果

一些生成的结构光图案样例如图所示。黑白条是投射的图案，彩色斑块是可视化了编码向量两两之间的相似度。按本文的说法，投射图案数量较少（K较小）时，图案本身的结构也会更多，这样才能更有效地区分投影和观测的编码向量至之间“谁是谁”。

![](../../.gitbook/assets2/image%20%2893%29.png)

## 4. Left-Right Comparative Recurrent Model for Stereo Matching

Tencent AI & NUS & UBC & UIUC，Oral

### 简介

左右一致性 \(left-right consistency\) 检测是一个有效的方法，但是传统方法严重依赖人工设计。这篇文章提出了全新的left-right compartive recurrent \(LRCR\) 模型来实现左右一致性检测，并同时估计视差 \(disparity\)。 在每一个recurrent的步骤里，模型对左右双目图像估计视差，然后实现实时左右比较（left-right comparision）来确定错误匹配的区域，这些区域可能是由错误的标注造成的。同时，算法介绍一个soft attention机制，通过使用学习到的错误匹配区域图，以便在下一个recurrent 步骤里更好地指导模型选择性地提升不稳定地区域。通过此方法，本文所提出的模型有效地提升了视差估计的效果，并且通过KITTI、Scene Flow 和 Middlebury 数据集得以验证。

现代深度学习方法依然受限于遮挡，反光，重复图案，错误匹配，无特征区域。然而，利用左右相互信息可以有效地确定不准确区域。针对此问题，本文提出了 left-right compartive recurrent \(LRCR\) 模型来整合左右一致性检测和视差生成到一个算法框架中一边相互提升效率并且避免了线下 \(offline\) 左右一致性检测所带来的缺陷。RNN可以使得LRCR模型逐渐的学习和利用左右视差一致性来提升视察估计。基于LSTM的RNN网络可以很好地整合前后视差信息。最终，双目图像逐渐收敛成稳定并且准确的估计。

文章主要贡献有以下三点：（1） 提出了深度RNN模型以利用双目实时左右一致性提升的视差估计（2）LRCR引入了一个attention机制以达到同时检测一致性和选取不稳定的区域细调（3）实验证明效果好，如图1所示。

![&#x56FE;1](../../.gitbook/assets2/image%20%2851%29.png)

### 算法

#### 1.LRCR model

LRCR的输入是matching cost volume \(H \* W \* d\_max\), 第三纬是patch平移 d 位置后在左右两图上的匹配误差\(matching cost\), matching cost volume 是通过一个 constant highway network 生成。图二中, I 为input cost volume, D 为 disparity map, 两个卷积LSTM独立生成左右 disparity map:  $$D_{left,t}$$和 $$D_{right,t}$$. $$D_{right,t}$$可以转换为左图视角的视差图$$D_{left',t}$$, 以方便的和$$D_{left,t}$$通过一个简单的卷积网络得到error map $$E_{left}$$，并对于右视图做同样处理。此 error map $$E_{left}$$作为soft attention 和下一帧图像的 matching cost volume 一起输入到下一帧的RNN步骤里面， 以便选择性地关注$$E_{left}$$标注出的错误区域。

![&#x56FE;2&#x3001;3](../../.gitbook/assets2/image%20%2862%29.png)

#### 2. 堆栈卷积LSTM \(stacked ConvLSTM\):

LRCR 采用两个并行的堆栈 ConvLSTM分别处理左右视图，每一个 stacked ConvLSTM 的输入是  matching cost volume 和上一帧的 error map 在第三维上堆积后的三维张量。具体的计算公式如下：

![](../../.gitbook/assets2/image%20%2827%29.png)

其中＊表示空间卷积操作， $$X_t$$是上面所提的输入张量， $$H_{t-1}$$ 和 $$C_{t-1}$$ 是上一帧的状态和细胞记忆。ConvLSTM的隐藏状态作为下一个ConvLSTM的输入使得多个ConvLSTM堆栈在一起，最后一个ConvLSTM的隐藏状态张量通过简单的卷积神经网络输出cost，之后将cost tensor 取负获得 score tensor。使用softmax来获取到每个视差值的概率，并最终加和得到预期视差值 $$d^* = \sum_{d=0}^{d_{max}} d \sigma(-c_d)$$ 。视差左右一致如上面图3所示。

#### 3. Loss function

Loss 采用L1 norm，为 $$\mathcal{L} = \frac{1}{N} \sum_{n=1}^N ||d_n - d_n^*||_1$$ 。

### 实验结果

提出的LRCR 模型可以通过加强关注错误区域，逐渐地减少视差估计的误差，而且在各个数据集上都获得了很好的结果。

